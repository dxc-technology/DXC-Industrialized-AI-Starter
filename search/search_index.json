{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DXC Industrialized AI Starter DXC Industrialized AI Starter makes it easy for you to deploy your AI algorithms (Industrialize). If you are a data scientist, working on an algorithm that you would like to deploy across the enterprise, DXC's Industrialized AI starter makes it easier for you to: Access, clean, and explore raw data Build data pipelines Run AI experiments Publish microservices Here is a short visual representation of the DXC Industrialized AI Starter. Installation To install and use the DXC AI Starter library, please use the below code snippet: 1. pip install DXC-Industrialized-AI-Starter 2. from dxc import ai Getting Started Access, Clean, and Explore Raw Data Use the library to access, clean, and explore your raw data. #Access raw data df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) df = ai.read_data_frame_from_local_json() df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() #Clean data: Imputes missing data, removes empty rows and columns, anonymizes text. raw_data = ai.clean_dataframe(df) #Explore raw data: ai.visualize_missing_data(raw_data) #visualizes relationships between all features in data. ai.explore_features(raw_data) #creates a visual display of missing data. ai.plot_distributions(raw_data) #creates a distribution graph for each column. #To visualize complete data as a report report = ai.explore_complete_data(df) report.to_notebook_iframe() Click here for details about Acess,clean,explore raw data. Build Data Pipelines Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. In order to get started, you need to first have an MongoDB account which you can signup for free and create a database \"connection_string\" and specify those details in the data_layer below. The following code connects to MongoDB and stores raw data for processing. #Insert data into MongoDB: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\", \"data_source\":\"<Source of your datset>\", \"cleaner\":\"<whether applied cleaner yes/no >\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) Once raw data is stored, you can run pipelines to transform the data. This code instructs the datastore on how to refine the output of raw data into something that can be used to train a machine-learning model. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) #refined data will be stored in pandas dataframe. Click here for details about building data pipeline. Run AI Experiments Use the DXC AI Starter to build and test algorithms. This code executes an experiment by running run_experiment() on experiment design. experiment_design = { #model options include ['tpot_regression()', 'tpot_classification()', 'timeseries'] \"model\": ai.tpot_regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design, verbose = False, max_time_mins = 5, max_eval_time_mins = 0.04, config_dict = None, warm_start = False, export_pipeline = True, scoring = None) Click here for details about run AI experiments. Publish Microservice The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. You must create an Algorithmia account to use. Below is the example for publishing a microservice. #trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } #publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) Click here for details about publishing microservice. Example Notebooks Here are example notebooks for individual models. These sample notebooks help to understand how to use each function, what parameters are expected for each function, and what will be the output of each function in a model. Reporting Issues If you find any issues, feel free to report them here with clear description of your issue.","title":"Welcome"},{"location":"#dxc-industrialized-ai-starter","text":"DXC Industrialized AI Starter makes it easy for you to deploy your AI algorithms (Industrialize). If you are a data scientist, working on an algorithm that you would like to deploy across the enterprise, DXC's Industrialized AI starter makes it easier for you to: Access, clean, and explore raw data Build data pipelines Run AI experiments Publish microservices Here is a short visual representation of the DXC Industrialized AI Starter.","title":"DXC Industrialized AI Starter"},{"location":"#installation","text":"To install and use the DXC AI Starter library, please use the below code snippet: 1. pip install DXC-Industrialized-AI-Starter 2. from dxc import ai","title":"Installation"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#access-clean-and-explore-raw-data","text":"Use the library to access, clean, and explore your raw data. #Access raw data df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) df = ai.read_data_frame_from_local_json() df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() #Clean data: Imputes missing data, removes empty rows and columns, anonymizes text. raw_data = ai.clean_dataframe(df) #Explore raw data: ai.visualize_missing_data(raw_data) #visualizes relationships between all features in data. ai.explore_features(raw_data) #creates a visual display of missing data. ai.plot_distributions(raw_data) #creates a distribution graph for each column. #To visualize complete data as a report report = ai.explore_complete_data(df) report.to_notebook_iframe() Click here for details about Acess,clean,explore raw data.","title":"Access, Clean, and Explore Raw Data"},{"location":"#build-data-pipelines","text":"Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. In order to get started, you need to first have an MongoDB account which you can signup for free and create a database \"connection_string\" and specify those details in the data_layer below. The following code connects to MongoDB and stores raw data for processing. #Insert data into MongoDB: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\", \"data_source\":\"<Source of your datset>\", \"cleaner\":\"<whether applied cleaner yes/no >\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) Once raw data is stored, you can run pipelines to transform the data. This code instructs the datastore on how to refine the output of raw data into something that can be used to train a machine-learning model. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) #refined data will be stored in pandas dataframe. Click here for details about building data pipeline.","title":"Build Data Pipelines"},{"location":"#run-ai-experiments","text":"Use the DXC AI Starter to build and test algorithms. This code executes an experiment by running run_experiment() on experiment design. experiment_design = { #model options include ['tpot_regression()', 'tpot_classification()', 'timeseries'] \"model\": ai.tpot_regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design, verbose = False, max_time_mins = 5, max_eval_time_mins = 0.04, config_dict = None, warm_start = False, export_pipeline = True, scoring = None) Click here for details about run AI experiments.","title":"Run AI Experiments"},{"location":"#publish-microservice","text":"The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. You must create an Algorithmia account to use. Below is the example for publishing a microservice. #trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } #publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) Click here for details about publishing microservice.","title":"Publish Microservice"},{"location":"#example-notebooks","text":"Here are example notebooks for individual models. These sample notebooks help to understand how to use each function, what parameters are expected for each function, and what will be the output of each function in a model.","title":"Example Notebooks"},{"location":"#reporting-issues","text":"If you find any issues, feel free to report them here with clear description of your issue.","title":"Reporting Issues"},{"location":"access_data/","text":"Access raw data Below code snippets help to bring in raw data from either a remote source or from your local machine into a data frame. df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) Get raw data from your local machine by opening up a file system browser, identifying a file, and importing the selected file. df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() Access CSV data ai.read_data_frame_from_remote_csv (csv_url, col_names = [], names=None, sep=',', delim_whitespace=False, header = 'infer', skiprows=None, error_bad_lines=True, encoding=None)__ It allows you to read character-delimited (commas, tabs, spaces) data from a URL. Expect csv_url parameter remaining all are optional. col_names: array-like, optional sep: str, default \u2018,\u2019 names: array-like, optional header: int, list of int, default \u2018infer\u2019 delim_whitespace: True or False skiprows: list-like, int or callable, optional error_bad_linesbool, default None encoding: str, optional - Encoding to use for UTF when reading/writing (ex. \u2018utf-8\u2019) ai.read_data_frame_from_local_csv(col_names = [], sep=',', delim_whitespace=False, header = 'infer', names = None, skiprows=None, error_bad_lines=True, encoding=None): This method allows you to import local character-delimited (commas, tabs, spaces) files. All parameters are optional. By default, the function will infer the header from the data, but an explicit header can be specified. Access JSON data ai.read_data_frame_from_remote_json(json_url) It reads JSON files from a URL and also flattened (in the case of nested data) the json data and cast into a pandas data frame. Access Excel data ai.read_data_frame_from_local_excel_file() This function allows you to import XLSX files. When the file explorer is launched, you must select an XLSX file or the function will result in an error.","title":"Access Data"},{"location":"access_data/#access-raw-data","text":"Below code snippets help to bring in raw data from either a remote source or from your local machine into a data frame. df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) Get raw data from your local machine by opening up a file system browser, identifying a file, and importing the selected file. df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file()","title":"Access raw data"},{"location":"access_data/#access-csv-data","text":"ai.read_data_frame_from_remote_csv (csv_url, col_names = [], names=None, sep=',', delim_whitespace=False, header = 'infer', skiprows=None, error_bad_lines=True, encoding=None)__ It allows you to read character-delimited (commas, tabs, spaces) data from a URL. Expect csv_url parameter remaining all are optional. col_names: array-like, optional sep: str, default \u2018,\u2019 names: array-like, optional header: int, list of int, default \u2018infer\u2019 delim_whitespace: True or False skiprows: list-like, int or callable, optional error_bad_linesbool, default None encoding: str, optional - Encoding to use for UTF when reading/writing (ex. \u2018utf-8\u2019) ai.read_data_frame_from_local_csv(col_names = [], sep=',', delim_whitespace=False, header = 'infer', names = None, skiprows=None, error_bad_lines=True, encoding=None): This method allows you to import local character-delimited (commas, tabs, spaces) files. All parameters are optional. By default, the function will infer the header from the data, but an explicit header can be specified.","title":"Access CSV data"},{"location":"access_data/#access-json-data","text":"ai.read_data_frame_from_remote_json(json_url) It reads JSON files from a URL and also flattened (in the case of nested data) the json data and cast into a pandas data frame.","title":"Access JSON data"},{"location":"access_data/#access-excel-data","text":"ai.read_data_frame_from_local_excel_file() This function allows you to import XLSX files. When the file explorer is launched, you must select an XLSX file or the function will result in an error.","title":"Access Excel data"},{"location":"clean_data/","text":"Clean Raw Data Below code snippet helps to clean raw data. clean_data = ai.clean_dataframe(df) This method imputes missing data, cleans the column headings, removes empty rows and columns, anonymizes text, and casts fields to their proper data type. Except for the data, every input field for the clean_dataframe is optional. By default, the method will not impute missing data. If instructed, the clean_dataframe will replace missing numeric fields with the mean value and replace missing categorical fields with the mode. ai.clean_dataframe(df, impute = False, text_fields = [], date_fields = [], numeric_fields = [], categorical_fields = []): The above function performs the following process: The column names are transformed to lower case and spaces are removed. If any of the rows or columns with total null values are removed completely. For text fields, the cleaner removes harmful characters, removes personal identifiers, converts to lowercase For object field types(categorical field), the KNN model is implemented with a default value of K as 2 to fill the missing values. KNN model imputes the missing values with the prediction based on all the other attributes. This makes data more reliable and meaningful as all the values are implemented using a prediction model against all the attributes. For the KNN model to run, the object datatypes are encoded into numerical values, and predictions are calculated based on the encoded values and after the predictions are made, values are decoded and converted back to original values for user convenience. For Numerical field types, the skewness of the column is calculated. If the skewness is within the range of -1 to 1, the mean of the column is used to impute the missing values otherwise median is used to impute the missing values.","title":"Clean Data"},{"location":"clean_data/#clean-raw-data","text":"Below code snippet helps to clean raw data. clean_data = ai.clean_dataframe(df) This method imputes missing data, cleans the column headings, removes empty rows and columns, anonymizes text, and casts fields to their proper data type. Except for the data, every input field for the clean_dataframe is optional. By default, the method will not impute missing data. If instructed, the clean_dataframe will replace missing numeric fields with the mean value and replace missing categorical fields with the mode. ai.clean_dataframe(df, impute = False, text_fields = [], date_fields = [], numeric_fields = [], categorical_fields = []): The above function performs the following process: The column names are transformed to lower case and spaces are removed. If any of the rows or columns with total null values are removed completely. For text fields, the cleaner removes harmful characters, removes personal identifiers, converts to lowercase For object field types(categorical field), the KNN model is implemented with a default value of K as 2 to fill the missing values. KNN model imputes the missing values with the prediction based on all the other attributes. This makes data more reliable and meaningful as all the values are implemented using a prediction model against all the attributes. For the KNN model to run, the object datatypes are encoded into numerical values, and predictions are calculated based on the encoded values and after the predictions are made, values are decoded and converted back to original values for user convenience. For Numerical field types, the skewness of the column is calculated. If the skewness is within the range of -1 to 1, the mean of the column is used to impute the missing values otherwise median is used to impute the missing values.","title":"Clean  Raw Data"},{"location":"contributing_guide/","text":"How to contribute We want to keep it as easy as possible to contribute to changes that get things working in your environment. There are a few guidelines that we need contributors to follow so that we can have a chance of keeping on top of things. Getting Started Make sure you have a GitHub account . Submit a ticket for your issue, assuming one does not already exist. DXC Industrialized AI Starter tickets are filed on the GitHub project in issues tab . Please try to clearly describe the issue, including the steps to reproduce any bug. Please include the story of \"why\" you want to do something. Fork the repository on GitHub. Glance at the Git Best Practices document, even if you don't read it all yet. Making Changes Create a topic branch for your work. You should branch off the master branch. Name your branch by the type of contribution and target: Generally, the type is bug , or feature , but if they don't fit pick something sensible. To create a topic branch based on master: git checkout master && git pull && git checkout -b bug/master/my_contribution Don't work directly on the master branch or any other core branch. Your pull request will be rejected unless it is on a topic branch. Every commit should do one thing, and only one thing. Having too many commits is better than having too few commits. Check for unnecessary whitespace with git diff --check before committing. Make sure your commit messages are in the proper format. If your commit fixes an issue, close it with your commit message (by appending, e.g., fixes #99999 , to the summary). Make sure you have added tests for your changes. Run all the tests to assure nothing else was accidentally broken. If possible, run the acceptance tests as well as the unit tests. You can always ask for help getting the tests working, or with writing tests. Click Here to view the basic test cases of AI starter library. Test cases file is located under Automate Test cases folder. These test cases should be passed by your new changes to ensure that existing basic functionalities are not affected by your latest commit. Please refer to Travis CI documentation for automate test case execution. Branching, and Where Changes Go Until a stable version of DXC Industrialized AI Starter is shipped, there is only one branch: master . All changes target that branch. Branch and Version Compatibility Any change to the DXC Industrialized AI Starter branch should strive as much as possible to be compatible with all released versions of DXC Industrialized AI Starter. We want to avoid multiple incompatible versions existing as much as possible. Until 1.0.0 we are willing to accept backward-incompatible changes if there is no possible way around it. Those changes MUST provide a migration strategy and, if possible, deprecation warnings about the older functionality. Right now any change committed to master must be considered \"live\". Submitting Changes Unless your contribution is trivial , ensure you have signed the Contributor License Agreement . Push your changes to a topic branch in your fork of the repository. Submit a pull request to the repository in the DXC organization. Update your ticket to mark that you have submitted code and are ready to be reviewed. Mentioning the issue number in the subject will make this happen through GitHub magic. A committer checks that the pull request is well-formed. If not, they will ask that it is fixed: It is on its own, appropriately named, branch. It was submitted to an appropriate target branch. It only has committed relevant to the specific issue. It has appropriate, clear, and effective commit messages. A committer can start a pull request specific discussion; at this point that covers: Reviewing the code for any obvious problems. Providing feedback based on personal experience on the subject. Testing relevant examples on an untested platform. Thoroughly stepping through the change to understand potential side-effects. Examining discrepancies between the original issue and the pull request. Using @mentioning to involve another committer in the discussion. Anyone can offer their assessment of a pull request, or be involved in the discussion, but keep in mind that this isn't the time to decide if the pull request is desired or not. The only reason it should be rejected at this point is if someone skipped the earlier steps in the process and submitted code before any discussion happened. Every review should add any specific changes required to the pull request: For comments on specific code, using GitHub line comments. For general changes, include them in the final assessment. Every review should end by specifying the type of review, and a vote: Good to merge. Good to merge with minor changes (which are specified, or line comments). Not good to merge without major changes (which are specified). Any committer can merge after there is a vote of \"good to merge\". Committers are trusted to do the right thing - you can merge your own code, but you should make sure you get an appropriate independent review. Most changes should not merge unless a code review has been completed. If the pull request is not reviewed within 14 days, you can ask any committer to execute the merge regardless: This can be blocked at any time by a single constructive vote against merging (\"Don't merge this, until you change...\") This is not stopped by a non-constructive vote (Don't merge this, I have not had a chance to look at it yet.\") The committer is encouraged to review before merging. Becoming a Committer DXC Industrialized AI Starter is an open project: any contributor can become a committer. Being a committer comes with great responsibility: your decisions directly shape the community, and the effectiveness, of the DXC Industrialized AI Starter project. You will probably invest more, and produce less, as a committer than a regular developer submitting pull requests. As a committer, your code is subject to the same review and commit restrictions as regular committers. You must exercise greater caution than most people in what you submit and include in the project. On the other hand, you have several additional responsibilities over and above those of a regular developer: 1. You are responsible for reviewing and voting on the inclusion of code from other developers. * You are responsible for giving constructive feedback that action can be taken on when code isn't quite there yet 2. You are responsible for ensuring that quality, tested code is committed. 3. You are responsible for ensuring that code merges into the appropriate branch. 4. You are responsible for ensuring that our community is diverse, accepting, and friendly. 5. You are responsible for voting in a timely fashion, where required. The best way to become a committer is to fulfill those requirements in the community, so that it is clear that approving you is just a formality. The process for adding a committer is: A candidate has demonstrated familiarity with the quality guidelines and coding standards by submitting at least two pull requests that are accepted without modification. The candidate is proposed by an existing committer. A formal vote is held on the project's private mailing list. Existing committers vote on the candidate: yes, accept them as a committer. no, do not accept them as a committer. If a majority of existing committers vote positively, the new committer is added to the public list of committers, and announced on the mailing list. Voting on adding a committer is absolutely private, and any feedback to candidates about why they were not accepted is at the option of the project leader. Removing Committers Removing a committer happens if they don't live up to their responsibilities, or if they violate the community standards. This is done by the project leader. The details of why are private, and will not be shared. Security issue notifications If you discover a potential security issue in this project we ask that you notify DXC Technology Security via email. Please do not create a public GitHub issue.","title":"How to Contribute"},{"location":"contributing_guide/#how-to-contribute","text":"We want to keep it as easy as possible to contribute to changes that get things working in your environment. There are a few guidelines that we need contributors to follow so that we can have a chance of keeping on top of things.","title":"How to contribute"},{"location":"contributing_guide/#getting-started","text":"Make sure you have a GitHub account . Submit a ticket for your issue, assuming one does not already exist. DXC Industrialized AI Starter tickets are filed on the GitHub project in issues tab . Please try to clearly describe the issue, including the steps to reproduce any bug. Please include the story of \"why\" you want to do something. Fork the repository on GitHub. Glance at the Git Best Practices document, even if you don't read it all yet.","title":"Getting Started"},{"location":"contributing_guide/#making-changes","text":"Create a topic branch for your work. You should branch off the master branch. Name your branch by the type of contribution and target: Generally, the type is bug , or feature , but if they don't fit pick something sensible. To create a topic branch based on master: git checkout master && git pull && git checkout -b bug/master/my_contribution Don't work directly on the master branch or any other core branch. Your pull request will be rejected unless it is on a topic branch. Every commit should do one thing, and only one thing. Having too many commits is better than having too few commits. Check for unnecessary whitespace with git diff --check before committing. Make sure your commit messages are in the proper format. If your commit fixes an issue, close it with your commit message (by appending, e.g., fixes #99999 , to the summary). Make sure you have added tests for your changes. Run all the tests to assure nothing else was accidentally broken. If possible, run the acceptance tests as well as the unit tests. You can always ask for help getting the tests working, or with writing tests. Click Here to view the basic test cases of AI starter library. Test cases file is located under Automate Test cases folder. These test cases should be passed by your new changes to ensure that existing basic functionalities are not affected by your latest commit. Please refer to Travis CI documentation for automate test case execution.","title":"Making Changes"},{"location":"contributing_guide/#branching-and-where-changes-go","text":"Until a stable version of DXC Industrialized AI Starter is shipped, there is only one branch: master . All changes target that branch.","title":"Branching, and Where Changes Go"},{"location":"contributing_guide/#branch-and-version-compatibility","text":"Any change to the DXC Industrialized AI Starter branch should strive as much as possible to be compatible with all released versions of DXC Industrialized AI Starter. We want to avoid multiple incompatible versions existing as much as possible. Until 1.0.0 we are willing to accept backward-incompatible changes if there is no possible way around it. Those changes MUST provide a migration strategy and, if possible, deprecation warnings about the older functionality. Right now any change committed to master must be considered \"live\".","title":"Branch and Version Compatibility"},{"location":"contributing_guide/#submitting-changes","text":"Unless your contribution is trivial , ensure you have signed the Contributor License Agreement . Push your changes to a topic branch in your fork of the repository. Submit a pull request to the repository in the DXC organization. Update your ticket to mark that you have submitted code and are ready to be reviewed. Mentioning the issue number in the subject will make this happen through GitHub magic. A committer checks that the pull request is well-formed. If not, they will ask that it is fixed: It is on its own, appropriately named, branch. It was submitted to an appropriate target branch. It only has committed relevant to the specific issue. It has appropriate, clear, and effective commit messages. A committer can start a pull request specific discussion; at this point that covers: Reviewing the code for any obvious problems. Providing feedback based on personal experience on the subject. Testing relevant examples on an untested platform. Thoroughly stepping through the change to understand potential side-effects. Examining discrepancies between the original issue and the pull request. Using @mentioning to involve another committer in the discussion. Anyone can offer their assessment of a pull request, or be involved in the discussion, but keep in mind that this isn't the time to decide if the pull request is desired or not. The only reason it should be rejected at this point is if someone skipped the earlier steps in the process and submitted code before any discussion happened. Every review should add any specific changes required to the pull request: For comments on specific code, using GitHub line comments. For general changes, include them in the final assessment. Every review should end by specifying the type of review, and a vote: Good to merge. Good to merge with minor changes (which are specified, or line comments). Not good to merge without major changes (which are specified). Any committer can merge after there is a vote of \"good to merge\". Committers are trusted to do the right thing - you can merge your own code, but you should make sure you get an appropriate independent review. Most changes should not merge unless a code review has been completed. If the pull request is not reviewed within 14 days, you can ask any committer to execute the merge regardless: This can be blocked at any time by a single constructive vote against merging (\"Don't merge this, until you change...\") This is not stopped by a non-constructive vote (Don't merge this, I have not had a chance to look at it yet.\") The committer is encouraged to review before merging.","title":"Submitting Changes"},{"location":"contributing_guide/#becoming-a-committer","text":"DXC Industrialized AI Starter is an open project: any contributor can become a committer. Being a committer comes with great responsibility: your decisions directly shape the community, and the effectiveness, of the DXC Industrialized AI Starter project. You will probably invest more, and produce less, as a committer than a regular developer submitting pull requests. As a committer, your code is subject to the same review and commit restrictions as regular committers. You must exercise greater caution than most people in what you submit and include in the project. On the other hand, you have several additional responsibilities over and above those of a regular developer: 1. You are responsible for reviewing and voting on the inclusion of code from other developers. * You are responsible for giving constructive feedback that action can be taken on when code isn't quite there yet 2. You are responsible for ensuring that quality, tested code is committed. 3. You are responsible for ensuring that code merges into the appropriate branch. 4. You are responsible for ensuring that our community is diverse, accepting, and friendly. 5. You are responsible for voting in a timely fashion, where required. The best way to become a committer is to fulfill those requirements in the community, so that it is clear that approving you is just a formality. The process for adding a committer is: A candidate has demonstrated familiarity with the quality guidelines and coding standards by submitting at least two pull requests that are accepted without modification. The candidate is proposed by an existing committer. A formal vote is held on the project's private mailing list. Existing committers vote on the candidate: yes, accept them as a committer. no, do not accept them as a committer. If a majority of existing committers vote positively, the new committer is added to the public list of committers, and announced on the mailing list. Voting on adding a committer is absolutely private, and any feedback to candidates about why they were not accepted is at the option of the project leader.","title":"Becoming a Committer"},{"location":"contributing_guide/#removing-committers","text":"Removing a committer happens if they don't live up to their responsibilities, or if they violate the community standards. This is done by the project leader. The details of why are private, and will not be shared.","title":"Removing Committers"},{"location":"contributing_guide/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify DXC Technology Security via email. Please do not create a public GitHub issue.","title":"Security issue notifications"},{"location":"data_pipeline/","text":"Build Data Pipelines Insert data into MongoDB Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. This code defines the meta-data needed to connect to Mongo DB Atlas and create a new datastore cluster. This is where you define basic information about the location of the cluster and the collection and database to use. Update this code with information appropriate to your project. To provide the information required in data_layer, you must: Create a MongoDb Atlas account Create a cluster Create a user Generate a connection string Note: When you configure the IP whitelist for your cluster, choose to allow a connection from anywhere. When creating the database connection string, choose the Python driver version 3.4 or later. Example: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\", \"data_source\":\"<Source of your datset>\", \"cleaner\":\"<whether applied cleaner yes/no >\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) ai.write_raw_data(data_layer, raw_data, date_fields = [ ]): This function handles Mongo DB Atlas automatically. Use write_raw_data function from ai library to convert Arrow dates to Strings data types It also transfers the raw data into the database and collection. It also stores metadata details, which is created as a separate collection, and it stores the information about each version along with their time of insertion. Version control in Mongo DB: DXC AI Starter is being equipped with the version control functionality, wherein the documents(data frames) inserted into the mongo are saved automatically as versions. The latest version is always in sync with the retrieved document. The versions are made with an underscore(_) and a number that is directly proportional to the number of times the document is changed. This helps in retrieving back the previous versions of the data. Build Pipeline Once raw data is stored, you can run pipelines to transform the data. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) ai.access_data_from_pipeline(write_raw_data, pipeline): This function instructs the datastore on how to refine the output of raw data into something that can be used to train a machine-learning model. The refined data will be stored in the df Pandas data frame. Make sure the output is what you want before continuing.","title":"Data Pipeline"},{"location":"data_pipeline/#build-data-pipelines","text":"","title":"Build Data Pipelines"},{"location":"data_pipeline/#insert-data-into-mongodb","text":"Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. This code defines the meta-data needed to connect to Mongo DB Atlas and create a new datastore cluster. This is where you define basic information about the location of the cluster and the collection and database to use. Update this code with information appropriate to your project. To provide the information required in data_layer, you must: Create a MongoDb Atlas account Create a cluster Create a user Generate a connection string Note: When you configure the IP whitelist for your cluster, choose to allow a connection from anywhere. When creating the database connection string, choose the Python driver version 3.4 or later. Example: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\", \"data_source\":\"<Source of your datset>\", \"cleaner\":\"<whether applied cleaner yes/no >\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) ai.write_raw_data(data_layer, raw_data, date_fields = [ ]): This function handles Mongo DB Atlas automatically. Use write_raw_data function from ai library to convert Arrow dates to Strings data types It also transfers the raw data into the database and collection. It also stores metadata details, which is created as a separate collection, and it stores the information about each version along with their time of insertion. Version control in Mongo DB: DXC AI Starter is being equipped with the version control functionality, wherein the documents(data frames) inserted into the mongo are saved automatically as versions. The latest version is always in sync with the retrieved document. The versions are made with an underscore(_) and a number that is directly proportional to the number of times the document is changed. This helps in retrieving back the previous versions of the data.","title":"Insert data into MongoDB"},{"location":"data_pipeline/#build-pipeline","text":"Once raw data is stored, you can run pipelines to transform the data. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) ai.access_data_from_pipeline(write_raw_data, pipeline): This function instructs the datastore on how to refine the output of raw data into something that can be used to train a machine-learning model. The refined data will be stored in the df Pandas data frame. Make sure the output is what you want before continuing.","title":"Build Pipeline"},{"location":"data_sets/","text":"Load Data Finding the right or sample dataset is one of the important tasks for those who are starting to work on Machine learning and AI. Though we have many datasets readily available finding the right dataset, loading, and using it can always be challenging especially for beginners. DXC Industrialized AI Starter makes this easy by providing a few data sets and functions to load them easily without any difficulties. Below code, snippet helps you to load and view the datasets from this library. # To know more details about the dataset print(ai.load_data_details('bike_sharing_data')) # To load the data sets into pandas dataframe df = ai.load_data('bike_sharing_data') df.head() For loading the datasets from this library we use below two functions. ai.load_data(filename): It takes a filename as input and loads that particular dataset into your notebook and return a pandas data frame. ai.load_data_details(filename): This function loads the dataset details and print information. This function takes a filename as input. The information retrieved helps the user to understand the dataset and the significance of those columns in the dataset. A list of datasets available in AI Starter is provided below. An example note on how to use these functions is placed under the \u201cExamples\u201d folder in the git repository. Available Datasets Below datasets are available in the DXC Industrialized AI Starter library. Dataset Name Filename to load New York city Airbnb data for the year 2019 ab_nyc_2019 Abalone data abalone Auto mobiles Imports data auto_imports Bike sharing Dataset bike_sharing Breast cancer data breast_cancer German credit data german Boston Housing dataset housing Optical recognition of handwritten digits hand_written_digits Breast cancer dataset obtained from the University of Wisconsin breast_cancer_wisconsin Horse colic data horse_colic Wine quality white winequality_white Boston Housing dataset with 163 attributes house_prices IBM employee attrition data hr_employee_attrition Johns Hopkins University Ionosphere data ionosphere Iris plants data iris Pima Indians diabetes data pima_indians_diabetes Sonar signals data sonar Wheat seeds data wheat_seeds Wine quality red winequality_red MNIST data mnist","title":"Load Data"},{"location":"data_sets/#load-data","text":"Finding the right or sample dataset is one of the important tasks for those who are starting to work on Machine learning and AI. Though we have many datasets readily available finding the right dataset, loading, and using it can always be challenging especially for beginners. DXC Industrialized AI Starter makes this easy by providing a few data sets and functions to load them easily without any difficulties. Below code, snippet helps you to load and view the datasets from this library. # To know more details about the dataset print(ai.load_data_details('bike_sharing_data')) # To load the data sets into pandas dataframe df = ai.load_data('bike_sharing_data') df.head() For loading the datasets from this library we use below two functions. ai.load_data(filename): It takes a filename as input and loads that particular dataset into your notebook and return a pandas data frame. ai.load_data_details(filename): This function loads the dataset details and print information. This function takes a filename as input. The information retrieved helps the user to understand the dataset and the significance of those columns in the dataset. A list of datasets available in AI Starter is provided below. An example note on how to use these functions is placed under the \u201cExamples\u201d folder in the git repository.","title":"Load Data"},{"location":"data_sets/#available-datasets","text":"Below datasets are available in the DXC Industrialized AI Starter library. Dataset Name Filename to load New York city Airbnb data for the year 2019 ab_nyc_2019 Abalone data abalone Auto mobiles Imports data auto_imports Bike sharing Dataset bike_sharing Breast cancer data breast_cancer German credit data german Boston Housing dataset housing Optical recognition of handwritten digits hand_written_digits Breast cancer dataset obtained from the University of Wisconsin breast_cancer_wisconsin Horse colic data horse_colic Wine quality white winequality_white Boston Housing dataset with 163 attributes house_prices IBM employee attrition data hr_employee_attrition Johns Hopkins University Ionosphere data ionosphere Iris plants data iris Pima Indians diabetes data pima_indians_diabetes Sonar signals data sonar Wheat seeds data wheat_seeds Wine quality red winequality_red MNIST data mnist","title":"Available Datasets"},{"location":"experiment/","text":"Run AI Experiments An experiment trains and tests a machine-learning model. The code in this section runs a model through a complete lifecycle and saves the final model to the local drive. Run the code that defines a machine-learning model and its lifecycle. Design an experiment and execute it. Most of the work of choosing features and specific model parameters will be done automatically. The code will also automatically score each option and return the options with the best predictive performance. Below is a example for run_experiment() function. experiment_design = { #model options include ['tpot_regression()', 'tpot_classification()', 'timeseries'] \"model\": ai.tpot_regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design, verbose = False, max_time_mins = 5, max_eval_time_mins = 0.04, config_dict = None, warm_start = False, export_pipeline = True, scoring = None) ai.run_experiment(experiment_design): This function executes an experiment on selected model. Update experiment_design with parameters that fit your project. The data parameter should be the same as the refined training data. The model parameter must be a model subclass. The labels parameter indicates the column of the data frame to be predicted. For the prediction model, the meta-data must describe the column to be predicted and the types for non-numeric columns. Further Information on Model The machine learning model provides the intelligent behavior that you will publish as a microservice. The code in this section provides you with options for the model. You must select a model capable of using df to learn the behavior specified in the design section of the data story. Run this function by defining each model type, then choose the model most appropriate for your data story. Each model adheres to the specifications of a model. This allows any of the models to run according to the standard model lifecycle defined in run_experiment. TPOT Classification/Regression: Machine learning is typically a very time-consuming and knowledge-intensive part of a data science problem. Auto-ml is not designed to replace the data scientist, but rather free to work on more important aspects of the complete problem, such as acquiring data and interpreting the model results. TPOT is a Python library developed for automatic machine learning feature preprocessing, model selection, and hyperparameter tuning. AI Starter has integrated TPOT as one of its Auto ML libraries. Please refer to the below parameter description for best utilization of Tpot classification and regression methods with AI Starter. verbose: True/False True - Prints more information and provide a progress bar; False - Prints nothing. max_time_mins: How many minutes the pipeline has to be optimized. The default value is 5 minutes. The maximum time better the results. max_eval_time_mins: How many minutes a single pipeline has to be evaluated. Setting this parameter to higher values will allow Auto_ml to evaluate more complex pipelines, but will also allow Auto_ml to run longer. Use this parameter to help prevent Auto_ml from wasting time on evaluating time-consuming pipelines. config_dict: Beyond the default configurations that come with Auto_ml, in some cases, it is useful to limit the algorithms and parameters that Auto_ml considers. For that reason, we allow users to provide Auto_ml with a custom configuration for its operators and parameters. For example: config_dict = {'sklearn.ensemble.GradientBoostingRegressor':{}} For more detailed examples and different configurations check here . warm_start: True/False This parameter lets you restart and continue to evaluate pipelines from where it left off in previous execution. export_pipeline: True/False This parameter automatically exports the corresponding Python code for the optimized pipeline to a python file and saves the python file and encoded data_file in your current directory. best_pipeline.py will contain the Python code for the optimized pipeline. The default value for this parameter is 'True'. We suggest not changing the value as the optimized pipeline code will help to evaluate your model using model explainability which will be one of the tasks to achieve the AI Forensics badge. scoring: This parameter is used to evaluate the quality of a given pipeline for the problem. By default, 'accuracy' is used for classification, and 'mean squared error' (MSE) is used for regression. The following built-in scoring functions can be used for Classification Problem: accuracy, adjusted_rand_score, average_precision, balanced_accuracy, f1, f1_macro, f1_micro, f1_samples, f1_weighted neg_log_loss, precision, etc. (suffixes apply as with \u2018f1\u2019) recall etc. (suffixes apply as with \u2018f1\u2019), \u2018Jaccard\u2019, etc. (suffixes apply as with \u2018f1\u2019) roc_auc, roc_auc_ovr, roc_auc_ovo, roc_auc_ovr_weighted, roc_auc_ovo_weighted The following built-in scoring functions can be used for Regression Problem: neg_median_absolute_error, neg_mean_absolute_error, neg_mean_squared_error, r2 If you need more knowledge on how to create custom scores please check here Timeseries Model The time-series model automates the process for predicting future values of a signal using a machine learning approach. It allows forecasting a time-series (or a signal) values for future in a fully automated way. The time series models that are used in this library are: Auto-Regressive model Simple exponential smoothing model Double exponential smoothing model ARIMA model The best model will be selected based on the RMSE value. Use this model when the design specification of the data story requires to predict future values based on time. Here is an example notebook for the time series model. Deep Learning Model DXC Industrialized AI starter has a Deep Learning model for image classification. Image classification is a supervised learning problem. A set of target classes are defined, and a model is trained with labeled images (train set). This trained model will identify and classifies new input into one of the target classes. This model can be used through three functions mentioned below: ai.create_training_data ai.split_normalize_data ai.image_classifier ai.create_training_data: This function reads each folder and each image from the path provided and will convert the image to an array. It will also resize your image to the size provided, the default size is 100. This function takes a list of image categories, image folder directory/path, and resizes size. This function return features and labels. ai.split_normalize_data: This function splits the data into train and test based on the size of the text provided. The default test size will be 0.20. Along with splitting the data to train and test, this function will also normalize the data. This function takes features, labels, test size (default 0.20), category count, and image size (default 100). This function returns the train and test sets. ai.image_classifier: This function compiles the model with the image size and number of categories provided. It adds the input, hidden, and output layers and compiles the model. Here returned model is trained with train data sets and further used for prediction. This example notebook helps you to understand how to use this model from the AI starter library. Clustering Model The clustering model is an unsupervised technique, which performs the clustering on the dataset depending on the attributes without knowing any information about the output. We aim to retrieve the hidden clusters from the data, our model performs the basic auto clustering based on the silhouette score. The Clustering models that are used in the library are: Affinity propagation K-Means DBSCAN The best method out of this is being selected based on the scores and the trained model of the same is returned as the output. Here is the example notebook for the clustering model.","title":"Experiment"},{"location":"experiment/#run-ai-experiments","text":"An experiment trains and tests a machine-learning model. The code in this section runs a model through a complete lifecycle and saves the final model to the local drive. Run the code that defines a machine-learning model and its lifecycle. Design an experiment and execute it. Most of the work of choosing features and specific model parameters will be done automatically. The code will also automatically score each option and return the options with the best predictive performance. Below is a example for run_experiment() function. experiment_design = { #model options include ['tpot_regression()', 'tpot_classification()', 'timeseries'] \"model\": ai.tpot_regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design, verbose = False, max_time_mins = 5, max_eval_time_mins = 0.04, config_dict = None, warm_start = False, export_pipeline = True, scoring = None) ai.run_experiment(experiment_design): This function executes an experiment on selected model. Update experiment_design with parameters that fit your project. The data parameter should be the same as the refined training data. The model parameter must be a model subclass. The labels parameter indicates the column of the data frame to be predicted. For the prediction model, the meta-data must describe the column to be predicted and the types for non-numeric columns. Further Information on Model The machine learning model provides the intelligent behavior that you will publish as a microservice. The code in this section provides you with options for the model. You must select a model capable of using df to learn the behavior specified in the design section of the data story. Run this function by defining each model type, then choose the model most appropriate for your data story. Each model adheres to the specifications of a model. This allows any of the models to run according to the standard model lifecycle defined in run_experiment.","title":"Run AI Experiments"},{"location":"experiment/#tpot-classificationregression","text":"Machine learning is typically a very time-consuming and knowledge-intensive part of a data science problem. Auto-ml is not designed to replace the data scientist, but rather free to work on more important aspects of the complete problem, such as acquiring data and interpreting the model results. TPOT is a Python library developed for automatic machine learning feature preprocessing, model selection, and hyperparameter tuning. AI Starter has integrated TPOT as one of its Auto ML libraries. Please refer to the below parameter description for best utilization of Tpot classification and regression methods with AI Starter. verbose: True/False True - Prints more information and provide a progress bar; False - Prints nothing. max_time_mins: How many minutes the pipeline has to be optimized. The default value is 5 minutes. The maximum time better the results. max_eval_time_mins: How many minutes a single pipeline has to be evaluated. Setting this parameter to higher values will allow Auto_ml to evaluate more complex pipelines, but will also allow Auto_ml to run longer. Use this parameter to help prevent Auto_ml from wasting time on evaluating time-consuming pipelines. config_dict: Beyond the default configurations that come with Auto_ml, in some cases, it is useful to limit the algorithms and parameters that Auto_ml considers. For that reason, we allow users to provide Auto_ml with a custom configuration for its operators and parameters. For example: config_dict = {'sklearn.ensemble.GradientBoostingRegressor':{}} For more detailed examples and different configurations check here . warm_start: True/False This parameter lets you restart and continue to evaluate pipelines from where it left off in previous execution. export_pipeline: True/False This parameter automatically exports the corresponding Python code for the optimized pipeline to a python file and saves the python file and encoded data_file in your current directory. best_pipeline.py will contain the Python code for the optimized pipeline. The default value for this parameter is 'True'. We suggest not changing the value as the optimized pipeline code will help to evaluate your model using model explainability which will be one of the tasks to achieve the AI Forensics badge. scoring: This parameter is used to evaluate the quality of a given pipeline for the problem. By default, 'accuracy' is used for classification, and 'mean squared error' (MSE) is used for regression. The following built-in scoring functions can be used for Classification Problem: accuracy, adjusted_rand_score, average_precision, balanced_accuracy, f1, f1_macro, f1_micro, f1_samples, f1_weighted neg_log_loss, precision, etc. (suffixes apply as with \u2018f1\u2019) recall etc. (suffixes apply as with \u2018f1\u2019), \u2018Jaccard\u2019, etc. (suffixes apply as with \u2018f1\u2019) roc_auc, roc_auc_ovr, roc_auc_ovo, roc_auc_ovr_weighted, roc_auc_ovo_weighted The following built-in scoring functions can be used for Regression Problem: neg_median_absolute_error, neg_mean_absolute_error, neg_mean_squared_error, r2 If you need more knowledge on how to create custom scores please check here","title":"TPOT Classification/Regression:"},{"location":"experiment/#timeseries-model","text":"The time-series model automates the process for predicting future values of a signal using a machine learning approach. It allows forecasting a time-series (or a signal) values for future in a fully automated way. The time series models that are used in this library are: Auto-Regressive model Simple exponential smoothing model Double exponential smoothing model ARIMA model The best model will be selected based on the RMSE value. Use this model when the design specification of the data story requires to predict future values based on time. Here is an example notebook for the time series model.","title":"Timeseries Model"},{"location":"experiment/#deep-learning-model","text":"DXC Industrialized AI starter has a Deep Learning model for image classification. Image classification is a supervised learning problem. A set of target classes are defined, and a model is trained with labeled images (train set). This trained model will identify and classifies new input into one of the target classes. This model can be used through three functions mentioned below: ai.create_training_data ai.split_normalize_data ai.image_classifier ai.create_training_data: This function reads each folder and each image from the path provided and will convert the image to an array. It will also resize your image to the size provided, the default size is 100. This function takes a list of image categories, image folder directory/path, and resizes size. This function return features and labels. ai.split_normalize_data: This function splits the data into train and test based on the size of the text provided. The default test size will be 0.20. Along with splitting the data to train and test, this function will also normalize the data. This function takes features, labels, test size (default 0.20), category count, and image size (default 100). This function returns the train and test sets. ai.image_classifier: This function compiles the model with the image size and number of categories provided. It adds the input, hidden, and output layers and compiles the model. Here returned model is trained with train data sets and further used for prediction. This example notebook helps you to understand how to use this model from the AI starter library.","title":"Deep Learning Model"},{"location":"experiment/#clustering-model","text":"The clustering model is an unsupervised technique, which performs the clustering on the dataset depending on the attributes without knowing any information about the output. We aim to retrieve the hidden clusters from the data, our model performs the basic auto clustering based on the silhouette score. The Clustering models that are used in the library are: Affinity propagation K-Means DBSCAN The best method out of this is being selected based on the scores and the trained model of the same is returned as the output. Here is the example notebook for the clustering model.","title":"Clustering Model"},{"location":"explore/","text":"Explore Data Explore raw data Below code, snippet helps to explore and visualize raw data. ai.visualize_missing_data(raw_data) ai.explore_features(raw_data) ai.plot_distributions(raw_data) ai.explore_features(df): It visualizes the relationships between all features in a given data frame. Areas of heat show closely-related features. This visualization is useful when trying to determine which features can be predicted and which features are needed to make the prediction. It is useful to explore the correlations between features in the raw data. Use this visualization to form a hypothesis about how raw data can be used. It may be necessary to enrich raw data with other features to increase the number and strength of correlations. ai.visualize_missing_data(df): It creates a visual display of missing data in a data frame. Each column of the data frame is shown as a column in the graph. Missing data is represented as horizontal lines in each column. This visualization is useful when determining whether or not to impute missing values or for determining whether or not the data is complete enough for analysis. It is useful to visualize missing fields in your raw data. Determine if imputing is necessary. ai.plot_distributions(df): It creates a distribution graph for each column in a given data frame. Graphs for data types that cannot be plotted on a distribution graph without refinement (types like dates), will show as blank in the output. This visualization is useful for determining skew or bias in the source data. Use plot_distributions to show the distributions for each feature in raw_data. Depending on raw data, this visualization may take several minutes to complete. Use the visualization to determine if there is a data skew that may prevent proper analysis or useful insight. Visualize complete data report = ai.explore_complete_data(df) report.to_notebook_iframe() ai.explore_complete_data(df): This function generates profile reports from a pandas DataFrame for exploratory data analysis. For each column, the following statistics are presented in an interactive HTML report. You can download that as an HTML report and share it with others easily. Here is the example notebook to know the usage of this function. Type inference: detect the types of columns in a DataFrame. Essentials: type, unique values, missing values Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness Most frequent values Histogram Correlations highlighting of highly correlated variables, Spearman, Pearson, and Kendall matrices Missing values matrix, count, heatmap, and dendrogram of missing values Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic), and blocks (ASCII) of text data. File and Image analysis extract file sizes, creation dates, and dimensions and scan for truncated images or those containing EXIF information.","title":"Visualization"},{"location":"explore/#explore-data","text":"","title":"Explore Data"},{"location":"explore/#explore-raw-data","text":"Below code, snippet helps to explore and visualize raw data. ai.visualize_missing_data(raw_data) ai.explore_features(raw_data) ai.plot_distributions(raw_data) ai.explore_features(df): It visualizes the relationships between all features in a given data frame. Areas of heat show closely-related features. This visualization is useful when trying to determine which features can be predicted and which features are needed to make the prediction. It is useful to explore the correlations between features in the raw data. Use this visualization to form a hypothesis about how raw data can be used. It may be necessary to enrich raw data with other features to increase the number and strength of correlations. ai.visualize_missing_data(df): It creates a visual display of missing data in a data frame. Each column of the data frame is shown as a column in the graph. Missing data is represented as horizontal lines in each column. This visualization is useful when determining whether or not to impute missing values or for determining whether or not the data is complete enough for analysis. It is useful to visualize missing fields in your raw data. Determine if imputing is necessary. ai.plot_distributions(df): It creates a distribution graph for each column in a given data frame. Graphs for data types that cannot be plotted on a distribution graph without refinement (types like dates), will show as blank in the output. This visualization is useful for determining skew or bias in the source data. Use plot_distributions to show the distributions for each feature in raw_data. Depending on raw data, this visualization may take several minutes to complete. Use the visualization to determine if there is a data skew that may prevent proper analysis or useful insight.","title":"Explore raw data"},{"location":"explore/#visualize-complete-data","text":"report = ai.explore_complete_data(df) report.to_notebook_iframe() ai.explore_complete_data(df): This function generates profile reports from a pandas DataFrame for exploratory data analysis. For each column, the following statistics are presented in an interactive HTML report. You can download that as an HTML report and share it with others easily. Here is the example notebook to know the usage of this function. Type inference: detect the types of columns in a DataFrame. Essentials: type, unique values, missing values Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness Most frequent values Histogram Correlations highlighting of highly correlated variables, Spearman, Pearson, and Kendall matrices Missing values matrix, count, heatmap, and dendrogram of missing values Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic), and blocks (ASCII) of text data. File and Image analysis extract file sizes, creation dates, and dimensions and scan for truncated images or those containing EXIF information.","title":"Visualize complete data"},{"location":"logging/","text":"Logging Functionality Logging is an important module in tracking the work or performance. Maintaining logs helps to trace the error, variable value, and other many uses. DXC industrialized AI starter also performs default logging for few crucial functions. We have provided the automatic logging functionality for the below major functions in this library. Build Data Pipelines Run AI Experiments Publish Microservice These auto logging is introduced to save the user build pipelines, inputs provided for run experiment function, and also to store the microservice design skeleton used for publishing models in Algorithmia. These logs can be referred back in the case to rerun or reuse any pipeline or microservice design details. Logs will be saved in the same location where the notebook with functions used. Refer to sample example notebook for reference to sample logs.","title":"Logging"},{"location":"logging/#logging-functionality","text":"Logging is an important module in tracking the work or performance. Maintaining logs helps to trace the error, variable value, and other many uses. DXC industrialized AI starter also performs default logging for few crucial functions. We have provided the automatic logging functionality for the below major functions in this library. Build Data Pipelines Run AI Experiments Publish Microservice These auto logging is introduced to save the user build pipelines, inputs provided for run experiment function, and also to store the microservice design skeleton used for publishing models in Algorithmia. These logs can be referred back in the case to rerun or reuse any pipeline or microservice design details. Logs will be saved in the same location where the notebook with functions used. Refer to sample example notebook for reference to sample logs.","title":"Logging Functionality"},{"location":"model_explainability/","text":"Model Explainability In the context of machine learning and artificial intelligence, Explainability is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. Model explainability is one of the most important problems in machine learning today. DXC Industrialized AI Starter helps you to understand the model explainability using interactive dashboards via SHAP - based explainer. global_explanation = ai.Global_Model_Explanation(model,x_train,x_test,feature_names = None,classes = None, explantion_data = None) ai.Explanation_Dashboard(global_explanation, model, x_train, x_test, explantion_data = None) To generate the model explainability, you need to pass your model, training data, test data to the functions. You can also optionally pass in feature names and output class names(classification) which will be used to make the explanations and visualizations more informative. Explanations will be generated default on the test data. If you pass the value of the explantion_data parameter as 'Training', then the explanation will be generated on training data. But with more examples, explanations will take longer although they may be more accurate. ai.Global_Model_Explanation: This function generates the overall model predictions and generates a dictionary of sorted feature importance names and values. ai.Explanation_Dashboard: This will generate an interactive visualization dashboard, you can investigate different aspects of your dataset and trained model via below four tab views: Model Performance Data Explorer Aggregate Feature Importance Individual Feature Importance SHAP explainer SHAP (SHapley Additive exPlanations) is a game-theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. Depending on the model, Model Explainer uses one of the below supported SHAP explainers. SHAP TreeExplainer SHAP DeepExplainer SHAP LinearExplainer SHAP KernelExplainer To know more details about SHAP explainer click here . Check out Examples to understand how to use each function, what parameters are expected for each function. Also check out shap , lime , interpret-community libraries to learn more about the Model explainability and its usage.","title":"Model Explainability"},{"location":"model_explainability/#model-explainability","text":"In the context of machine learning and artificial intelligence, Explainability is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. Model explainability is one of the most important problems in machine learning today. DXC Industrialized AI Starter helps you to understand the model explainability using interactive dashboards via SHAP - based explainer. global_explanation = ai.Global_Model_Explanation(model,x_train,x_test,feature_names = None,classes = None, explantion_data = None) ai.Explanation_Dashboard(global_explanation, model, x_train, x_test, explantion_data = None) To generate the model explainability, you need to pass your model, training data, test data to the functions. You can also optionally pass in feature names and output class names(classification) which will be used to make the explanations and visualizations more informative. Explanations will be generated default on the test data. If you pass the value of the explantion_data parameter as 'Training', then the explanation will be generated on training data. But with more examples, explanations will take longer although they may be more accurate. ai.Global_Model_Explanation: This function generates the overall model predictions and generates a dictionary of sorted feature importance names and values. ai.Explanation_Dashboard: This will generate an interactive visualization dashboard, you can investigate different aspects of your dataset and trained model via below four tab views: Model Performance Data Explorer Aggregate Feature Importance Individual Feature Importance","title":"Model Explainability"},{"location":"model_explainability/#shap-explainer","text":"SHAP (SHapley Additive exPlanations) is a game-theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. Depending on the model, Model Explainer uses one of the below supported SHAP explainers. SHAP TreeExplainer SHAP DeepExplainer SHAP LinearExplainer SHAP KernelExplainer To know more details about SHAP explainer click here . Check out Examples to understand how to use each function, what parameters are expected for each function. Also check out shap , lime , interpret-community libraries to learn more about the Model explainability and its usage.","title":"SHAP explainer"},{"location":"publish_microservice/","text":"Publish Microservice The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. To provide the information required to design the microservice, you must: create an Algorithmia account create an API key with BOTH \"Read & Write Data\" and \"Manage Algorithms\" permissions enabled create an algorithm user name trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } # publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) publish_microservice(microservice_design, trained_model): The code in this section prepares an execution environment for the microservice, builds a microservice using the machine-learning model, deploys the microservice into the execution environment, and publishes an API endpoint for the microservice. The work of creating the microservice and deploying it will be done automatically. The code will also automatically handle the source code repository management. This code defines the parameters needed to build and deploy a microservice-based on the trained model. Update microservice_design with parameters appropriate for your project. The parameters must contain valid keys, namespaces, and model paths from Algorithmia.","title":"Publish Microservice"},{"location":"publish_microservice/#publish-microservice","text":"The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. To provide the information required to design the microservice, you must: create an Algorithmia account create an API key with BOTH \"Read & Write Data\" and \"Manage Algorithms\" permissions enabled create an algorithm user name trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } # publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) publish_microservice(microservice_design, trained_model): The code in this section prepares an execution environment for the microservice, builds a microservice using the machine-learning model, deploys the microservice into the execution environment, and publishes an API endpoint for the microservice. The work of creating the microservice and deploying it will be done automatically. The code will also automatically handle the source code repository management. This code defines the parameters needed to build and deploy a microservice-based on the trained model. Update microservice_design with parameters appropriate for your project. The parameters must contain valid keys, namespaces, and model paths from Algorithmia.","title":"Publish Microservice"},{"location":"reinforcement_learning/","text":"Reinforcement Learning Set up the development environment This code installs all the packages you will need. Run it first. It should take 30 seconds or so to complete. If you get missing module errors later, it may be because you have not run this code. Restart the runtime/session after executing the below code. ! pip install git+https://github.com/dxc-technology/DXC-Industrialized-AI-Starter.git -qq from dxc import rl from gym import spaces import numpy as np import gym Reinforcement Learning Basics Reinforcement learning is machine learning using rewards given to an agent acting in an environment. Instead of learning from historical data, the agent learns how to maneuver through the environment by receiving positive or negative rewards depending on the actions that it takes. Train an OpenAI Gym environment Gym is a toolkit used for reinforcement learning created by OpenAI that includes several premade environments to test your models on. View OpenAI Gym environments Models There are several models that can be used to train an agent in an environment. The model that needs to be used can be generally determined by the type of actions an agent can take. If a discrete set of actions are given, DQN or SARSA can be used. A discrete set of actions are actions that you can count. Examples of this include 3 actions for Rock, Paper, Scissors or 2 actions in TicTacToe for X and O. If the action space were between 1 to 5, the actions would be 1, 2, 3, 4, and 5. If a continous set of actions are given, DDPG can be used. A continuous set of actions are actions that you can measure. Examples of this include how fast a car should run (mph) or how likely a card will appear in a board game (%). If the action space were between 1 to 5, the actions would be all numbers between 1 to 5, including all decimal values. Calling the Reinforcement Learning Helper rl_helper() accepts 2 parameters to run. The first parameter is the environment. The second parameter is the type of model the environment will train in. There are extra parameters that can be defined, but aren't necessary. These are used to further refine your model. Listing them down and defining the default values in parenthesis after the parameter name, the parameters that can be set for both discrete and continuous environments are: - steps (50000): number of episodes that the model will run for - saved_model_name (model): name of the saved model files - visualize (False): boolean (True or False) if there is visualization for the model, set True to display it The other parameters are only used for discrete environments. - test_steps (5): number of episodes the model will test - critic_hidden_layers (3): number of critic hidden layers - hidden_layers (3): number of hidden layers To call a gym environment, simply use the gym.make() function and pass the name of the gym environment. You can get the name of the gym environment here. Continuous Example In the following example code, the Pendulum environment is used. This is an environment where the goal of the agent is to balance a pendulum upright. Unlike the cartpole example, the actions set for this environment are continuous, so DDPG will be used to train it. env = gym.make(\"Pendulum-v0\") rl.rl_helper(env=env, model_name=\"DDPG\", saved_model_name=\"pendulum_model\", steps=1000) Discrete Example In the following example code, the Cart Pole environment is used. This is an environment where the goal of the agent (which is a cart) is to balance a pole. The model used is DQN (discrete actions) since the actions that the cart can take is move left or right. SARSA may also be used since it is a discrete environment. import gym gym_env = gym.make(\"CartPole-v0\") rl.rl_helper(env=gym_env, model_name=\"DQN\", #SARSA steps=25000, test_steps=5, visualize=False, saved_model_name=\"cartpole_model\", critic_hidden_layers=2, hidden_layers=2) Making your own environment Now that you've learned how to use the rl_helper() , you can explore how to create your own environment to train your own agent. Each environment has 3 main parts: __init__() is where the variables describing the environment is initialized. This includes variables that define what an agent can do and observe. This also includes the variables that define how the environment looks as the agent is going through it. It is important to define what the agent can observe since this is what the agent will base his choices in his action on. For example, in an environment where a car is running, if the road is slippery, the agent might want to choose a slower speed. In __init__() , it is important to define the following variables in the environment: action_space initializes the number of actions that are defined. observation_space initializes the number of variables that is observed by the agent total_reward initializes and later stores the rewards that the agent has isDone initializes and stores whether or not the agent is done going through the environment The rest of the variables defined in init are variables that just describe what the environment looks like to the agent. reset() is where the variables of the environment are reset when the environment is ran again. Generally, it will just look like __init__() since we are just setting the variables to its original values again. This is done because the agent will train in the environment multiple times. To make sure each run independent of each other, the variables need to be reset to their original values. It is important to note that reset() should return what the agent can observe. step() is where the steps that the agent takes through the environment is defined.","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#reinforcement-learning","text":"","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#set-up-the-development-environment","text":"This code installs all the packages you will need. Run it first. It should take 30 seconds or so to complete. If you get missing module errors later, it may be because you have not run this code. Restart the runtime/session after executing the below code. ! pip install git+https://github.com/dxc-technology/DXC-Industrialized-AI-Starter.git -qq from dxc import rl from gym import spaces import numpy as np import gym","title":"Set up the development environment"},{"location":"reinforcement_learning/#reinforcement-learning-basics","text":"Reinforcement learning is machine learning using rewards given to an agent acting in an environment. Instead of learning from historical data, the agent learns how to maneuver through the environment by receiving positive or negative rewards depending on the actions that it takes.","title":"Reinforcement Learning Basics"},{"location":"reinforcement_learning/#train-an-openai-gym-environment","text":"Gym is a toolkit used for reinforcement learning created by OpenAI that includes several premade environments to test your models on. View OpenAI Gym environments","title":"Train an OpenAI Gym environment"},{"location":"reinforcement_learning/#models","text":"There are several models that can be used to train an agent in an environment. The model that needs to be used can be generally determined by the type of actions an agent can take. If a discrete set of actions are given, DQN or SARSA can be used. A discrete set of actions are actions that you can count. Examples of this include 3 actions for Rock, Paper, Scissors or 2 actions in TicTacToe for X and O. If the action space were between 1 to 5, the actions would be 1, 2, 3, 4, and 5. If a continous set of actions are given, DDPG can be used. A continuous set of actions are actions that you can measure. Examples of this include how fast a car should run (mph) or how likely a card will appear in a board game (%). If the action space were between 1 to 5, the actions would be all numbers between 1 to 5, including all decimal values.","title":"Models"},{"location":"reinforcement_learning/#calling-the-reinforcement-learning-helper","text":"rl_helper() accepts 2 parameters to run. The first parameter is the environment. The second parameter is the type of model the environment will train in. There are extra parameters that can be defined, but aren't necessary. These are used to further refine your model. Listing them down and defining the default values in parenthesis after the parameter name, the parameters that can be set for both discrete and continuous environments are: - steps (50000): number of episodes that the model will run for - saved_model_name (model): name of the saved model files - visualize (False): boolean (True or False) if there is visualization for the model, set True to display it The other parameters are only used for discrete environments. - test_steps (5): number of episodes the model will test - critic_hidden_layers (3): number of critic hidden layers - hidden_layers (3): number of hidden layers To call a gym environment, simply use the gym.make() function and pass the name of the gym environment. You can get the name of the gym environment here.","title":"Calling the Reinforcement Learning Helper"},{"location":"reinforcement_learning/#continuous-example","text":"In the following example code, the Pendulum environment is used. This is an environment where the goal of the agent is to balance a pendulum upright. Unlike the cartpole example, the actions set for this environment are continuous, so DDPG will be used to train it. env = gym.make(\"Pendulum-v0\") rl.rl_helper(env=env, model_name=\"DDPG\", saved_model_name=\"pendulum_model\", steps=1000)","title":"Continuous Example"},{"location":"reinforcement_learning/#discrete-example","text":"In the following example code, the Cart Pole environment is used. This is an environment where the goal of the agent (which is a cart) is to balance a pole. The model used is DQN (discrete actions) since the actions that the cart can take is move left or right. SARSA may also be used since it is a discrete environment. import gym gym_env = gym.make(\"CartPole-v0\") rl.rl_helper(env=gym_env, model_name=\"DQN\", #SARSA steps=25000, test_steps=5, visualize=False, saved_model_name=\"cartpole_model\", critic_hidden_layers=2, hidden_layers=2)","title":"Discrete Example"},{"location":"reinforcement_learning/#making-your-own-environment","text":"Now that you've learned how to use the rl_helper() , you can explore how to create your own environment to train your own agent. Each environment has 3 main parts: __init__() is where the variables describing the environment is initialized. This includes variables that define what an agent can do and observe. This also includes the variables that define how the environment looks as the agent is going through it. It is important to define what the agent can observe since this is what the agent will base his choices in his action on. For example, in an environment where a car is running, if the road is slippery, the agent might want to choose a slower speed. In __init__() , it is important to define the following variables in the environment: action_space initializes the number of actions that are defined. observation_space initializes the number of variables that is observed by the agent total_reward initializes and later stores the rewards that the agent has isDone initializes and stores whether or not the agent is done going through the environment The rest of the variables defined in init are variables that just describe what the environment looks like to the agent. reset() is where the variables of the environment are reset when the environment is ran again. Generally, it will just look like __init__() since we are just setting the variables to its original values again. This is done because the agent will train in the environment multiple times. To make sure each run independent of each other, the variables need to be reset to their original values. It is important to note that reset() should return what the agent can observe. step() is where the steps that the agent takes through the environment is defined.","title":"Making your own environment"}]}