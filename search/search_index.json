{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DXC Industrialized AI Starter DXC Indusrialized AI Starter makes it easy for you to deploy your AI algorithms (Industrialize). If you are a data scientist, working on an algorithm that you would like to deploy across the enterprise, DXC's Industrialized AI starter makes it easier for you to: Access, clean, and explore raw data Build data pipelines Run AI experiments Publish microservices Installation In order to install and use the DXC AI Starter library, please use the below code snippet: 1. pip install DXC-Industrialized-AI-Starter 2. from dxc import ai Getting Started Access, Clean, and Explore Raw Data Use the library to access, clean, and explore your raw data. #Access raw data df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) df = ai.read_data_frame_from_local_json() df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() #Clean data: Imputes missing data, removes empty rows and columns, anonymizes text. raw_data = ai.clean_dataframe(df) #Explore raw data: ai.visualize_missing_data(raw_data) #visualizes relationships between all features in data. ai.explore_features(raw_data) #creates a visual display of missing data. ai.plot_distributions(raw_data) #creates a distribution graph for each column. Click here for details about Acess,clean,explore raw data. Build Data Pipelines Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. In order to get started, you need to first have an MongoDB account which you can signup for free and create a database \"connection_string\" and specify those details in the data_layer below. The following code connects to MongoDB and stores raw data for processing. #Insert data into MongoDB: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) Once raw data is stored, you can run pipelines to transform the data. This code instructs the data store on how to refine the output of raw data into something that can be used to train a machine-learning model. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) #refined data will be stored in pandas dataframe. Click here for details about building data pipeline. Run AI Experiments Use the DXC AI Starter to build and test algorithms. This code executes an experiment by running run_experiment() on an experiment design. experiment_design = { #model options include ['regression()', 'classification()'] \"model\": ai.regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design) Click here for details about run AI experiments. Publish Microservice The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. You must create an Algorithmia account to use. Below is the example for publishing a microservice. #trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } #publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) Click here for details about publishing microservice. Docs For detailed and complete documentation, please click here Example of colab notebook Here is an detailed and in-depth example of DXC Indusrialized AI Starter library usage. Contributing Guide To know more about the contribution and guidelines please click here Reporting Issues If you find any issues, feel free to report them here with clear description of your issue.","title":"Welcome"},{"location":"#dxc-industrialized-ai-starter","text":"DXC Indusrialized AI Starter makes it easy for you to deploy your AI algorithms (Industrialize). If you are a data scientist, working on an algorithm that you would like to deploy across the enterprise, DXC's Industrialized AI starter makes it easier for you to: Access, clean, and explore raw data Build data pipelines Run AI experiments Publish microservices","title":"DXC Industrialized AI Starter"},{"location":"#installation","text":"In order to install and use the DXC AI Starter library, please use the below code snippet: 1. pip install DXC-Industrialized-AI-Starter 2. from dxc import ai","title":"Installation"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#access-clean-and-explore-raw-data","text":"Use the library to access, clean, and explore your raw data. #Access raw data df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) df = ai.read_data_frame_from_local_json() df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() #Clean data: Imputes missing data, removes empty rows and columns, anonymizes text. raw_data = ai.clean_dataframe(df) #Explore raw data: ai.visualize_missing_data(raw_data) #visualizes relationships between all features in data. ai.explore_features(raw_data) #creates a visual display of missing data. ai.plot_distributions(raw_data) #creates a distribution graph for each column. Click here for details about Acess,clean,explore raw data.","title":"Access, Clean, and Explore Raw Data"},{"location":"#build-data-pipelines","text":"Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. In order to get started, you need to first have an MongoDB account which you can signup for free and create a database \"connection_string\" and specify those details in the data_layer below. The following code connects to MongoDB and stores raw data for processing. #Insert data into MongoDB: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) Once raw data is stored, you can run pipelines to transform the data. This code instructs the data store on how to refine the output of raw data into something that can be used to train a machine-learning model. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) #refined data will be stored in pandas dataframe. Click here for details about building data pipeline.","title":"Build Data Pipelines"},{"location":"#run-ai-experiments","text":"Use the DXC AI Starter to build and test algorithms. This code executes an experiment by running run_experiment() on an experiment design. experiment_design = { #model options include ['regression()', 'classification()'] \"model\": ai.regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design) Click here for details about run AI experiments.","title":"Run AI Experiments"},{"location":"#publish-microservice","text":"The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. You must create an Algorithmia account to use. Below is the example for publishing a microservice. #trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } #publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) Click here for details about publishing microservice.","title":"Publish Microservice"},{"location":"#docs","text":"For detailed and complete documentation, please click here","title":"Docs"},{"location":"#example-of-colab-notebook","text":"Here is an detailed and in-depth example of DXC Indusrialized AI Starter library usage.","title":"Example of colab notebook"},{"location":"#contributing-guide","text":"To know more about the contribution and guidelines please click here","title":"Contributing Guide"},{"location":"#reporting-issues","text":"If you find any issues, feel free to report them here with clear description of your issue.","title":"Reporting Issues"},{"location":"access_clean_explore/","text":"Access, Clean and Explore Access raw data Below code snippets help to bring in raw data from either a remote source or from your local machine into a dataframe. df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) Get raw data from your local machine by opening up a file system browser, identifying a file, and importing the selected file. df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() read_data_frame_from_remote_json(json_url): It reads JSON files from a URL and also flattened (in the case of nested data) the json data and cast into a pandas data frame. read_data_frame_from_remote_csv(csv_url, col_names = [], delim_whitespace=False, header = 'infer'): It allows you to read character-delimited (commas, tabs, spaces) data from a URL. Expect csv_url parameter remaining all are optional. read_data_frame_from_local_csv(col_names = [], delim_whitespace=False, header = 'infer'): This method allows you to import local character-delimited (commas, tabs, spaces) files. All parameters are optional. By default, the function will infer the header from the data, but an explicit header can be specified. read_data_frame_from_local_excel_file(): This function allows you to import XLSX files. When the file explorer is launched, you must select an XLSX file or the function will result in an error. Clean raw data Below code snippet helps to clean raw data. raw_data = ai.clean_dataframe(df) clean_dataframe(df, impute = False, text_fields = [], date_fields = [], numeric_fields = [], categorical_fields = []): This method imputes missing data, cleans the column headings, removes empty rows and columns, anonymizes text, and casts fields to their proper data type. Except for the data, every input field for clean_dataframe is optional. By default the method will not impute missing data. If instructed, clean_dataframe will replace missing numeric fields with the mean value and replace missing categorical fields with the mode. Explore raw data Below code snippet helps to explore and visualize raw data. ai.visualize_missing_data(raw_data) ai.explore_features(raw_data) ai.plot_distributions(raw_data) explore_features(df): It visualizes the relationships between all features in a given data frame. Areas of heat show closely-related features. This visualization is useful when trying to determine which features can be predicted and which features are needed to make the prediction. It is useful to explore the correlations between features in the raw data. Use this visualization to form a hypothesis about how the raw data can be used. It may be necessary to enrich raw data with other features to increase the number and strength of correlations. visualize_missing_data(df): It creates a visual display of missing data in a data frame. Each column of the data frame is shown as a column in the graph. Missing data is represented as horizontal lines in each column. This visualization is useful when determining whether or not to impute missing values or for determining whether or not the data is complete enough for analysis. It is useful to visualize missing fields in your raw data. Determine if imputing is necessary. plot_distributions(df): It creates a distribution graph for each column in a given data frame. Graphs for data types that cannot be plotted on a distribution graph without refinement (types like dates), will show as blank in the output. This visualization is useful for determining skew or bias in the source data. Use plot_distributions to show the distributions for each feature in raw_data. Depending on raw data, this visualization may take several minutes to complete. Use the visualization to determine if there is a data skew that may prevent proper analysis or useful insight.","title":"Access,Clean and Explore"},{"location":"access_clean_explore/#access-clean-and-explore","text":"","title":"Access, Clean and Explore"},{"location":"access_clean_explore/#access-raw-data","text":"Below code snippets help to bring in raw data from either a remote source or from your local machine into a dataframe. df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) Get raw data from your local machine by opening up a file system browser, identifying a file, and importing the selected file. df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() read_data_frame_from_remote_json(json_url): It reads JSON files from a URL and also flattened (in the case of nested data) the json data and cast into a pandas data frame. read_data_frame_from_remote_csv(csv_url, col_names = [], delim_whitespace=False, header = 'infer'): It allows you to read character-delimited (commas, tabs, spaces) data from a URL. Expect csv_url parameter remaining all are optional. read_data_frame_from_local_csv(col_names = [], delim_whitespace=False, header = 'infer'): This method allows you to import local character-delimited (commas, tabs, spaces) files. All parameters are optional. By default, the function will infer the header from the data, but an explicit header can be specified. read_data_frame_from_local_excel_file(): This function allows you to import XLSX files. When the file explorer is launched, you must select an XLSX file or the function will result in an error.","title":"Access raw data"},{"location":"access_clean_explore/#clean-raw-data","text":"Below code snippet helps to clean raw data. raw_data = ai.clean_dataframe(df) clean_dataframe(df, impute = False, text_fields = [], date_fields = [], numeric_fields = [], categorical_fields = []): This method imputes missing data, cleans the column headings, removes empty rows and columns, anonymizes text, and casts fields to their proper data type. Except for the data, every input field for clean_dataframe is optional. By default the method will not impute missing data. If instructed, clean_dataframe will replace missing numeric fields with the mean value and replace missing categorical fields with the mode.","title":"Clean raw data"},{"location":"access_clean_explore/#explore-raw-data","text":"Below code snippet helps to explore and visualize raw data. ai.visualize_missing_data(raw_data) ai.explore_features(raw_data) ai.plot_distributions(raw_data) explore_features(df): It visualizes the relationships between all features in a given data frame. Areas of heat show closely-related features. This visualization is useful when trying to determine which features can be predicted and which features are needed to make the prediction. It is useful to explore the correlations between features in the raw data. Use this visualization to form a hypothesis about how the raw data can be used. It may be necessary to enrich raw data with other features to increase the number and strength of correlations. visualize_missing_data(df): It creates a visual display of missing data in a data frame. Each column of the data frame is shown as a column in the graph. Missing data is represented as horizontal lines in each column. This visualization is useful when determining whether or not to impute missing values or for determining whether or not the data is complete enough for analysis. It is useful to visualize missing fields in your raw data. Determine if imputing is necessary. plot_distributions(df): It creates a distribution graph for each column in a given data frame. Graphs for data types that cannot be plotted on a distribution graph without refinement (types like dates), will show as blank in the output. This visualization is useful for determining skew or bias in the source data. Use plot_distributions to show the distributions for each feature in raw_data. Depending on raw data, this visualization may take several minutes to complete. Use the visualization to determine if there is a data skew that may prevent proper analysis or useful insight.","title":"Explore raw data"},{"location":"data_pipeline/","text":"Build Data Pipelines Insert data into MongoDB Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. This code defines the meta-data needed to connect to Mongo DB Atlas and create a new data store cluster. This is where you define basic information about the location of the cluster and the collection and database to use. Update this code with information appropriate to your project. In order to provide the information required in data_layer, you must: Create a MongoDb Atlas account Create a cluster Create a user Generate a connection string Note: When you configure the IP whitelist for your cluster, choose to allow a connection from anywhere. When creating the database connection string, choose the Python driver version 3.4 or later. Example: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) write_raw_data(data_layer, raw_data, date_fields = [ ]): This function handles Mongo DB Atlas automatically. Use write_raw_data function from ai library to convert Arrow dates to Strings data types It also transfers the raw data into the database and collection. Build Pipeline Once raw data is stored, you can run pipelines to transform the data. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) access_data_from_pipeline(write_raw_data, pipeline): This function instructs the data store on how to refine the output of raw data into something that can be used to train a machine-learning model. The refined data will be stored in the df Pandas dataframe. Make sure the output is what you want before continuing.","title":"Data Pipeline"},{"location":"data_pipeline/#build-data-pipelines","text":"","title":"Build Data Pipelines"},{"location":"data_pipeline/#insert-data-into-mongodb","text":"Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. This code defines the meta-data needed to connect to Mongo DB Atlas and create a new data store cluster. This is where you define basic information about the location of the cluster and the collection and database to use. Update this code with information appropriate to your project. In order to provide the information required in data_layer, you must: Create a MongoDb Atlas account Create a cluster Create a user Generate a connection string Note: When you configure the IP whitelist for your cluster, choose to allow a connection from anywhere. When creating the database connection string, choose the Python driver version 3.4 or later. Example: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) write_raw_data(data_layer, raw_data, date_fields = [ ]): This function handles Mongo DB Atlas automatically. Use write_raw_data function from ai library to convert Arrow dates to Strings data types It also transfers the raw data into the database and collection.","title":"Insert data into MongoDB"},{"location":"data_pipeline/#build-pipeline","text":"Once raw data is stored, you can run pipelines to transform the data. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) access_data_from_pipeline(write_raw_data, pipeline): This function instructs the data store on how to refine the output of raw data into something that can be used to train a machine-learning model. The refined data will be stored in the df Pandas dataframe. Make sure the output is what you want before continuing.","title":"Build Pipeline"},{"location":"experiment/","text":"Run AI Experiments An experiment trains and tests a machine-learning model. The code in this section runs a model through a complete lifecycle and saves the final model to the local drive. Run the code that defines a machine-learning model and its lifecycle. Design an experiment and execute it. Most of the work of choosing features and specific model parameters will be done automatically. The code will also automatically score each option and return the options with the best predictive performance. Below is a example for run_experiment() function. experiment_design = { #model options include ['regression()', 'classification()'] \"model\": ai.regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design) run_experiment(experiment_design): This function executes an experiment on selected model. Update experiment_design with parameters that fit your project. The data parameter should be same as the refined training data. The model parameter must be a model subclass. The labels parameter indicates the column of the data dataframe to be predicted. For the prediction model, the meta-data must describe the column to be predicted and the types for non-numeric columns. Further Information on Model Machine learning model provides the intelligent behavior that you will publish as a microservice. The code in this section provides you with options for the model. You must select a model capable of using df to learn the behavior specified in the design section of the datastory. Run this function by defining each model type, then choose the model most appropriate for your datastory. Each model adheres to the specifications of a model. This allows any of the models to run according to the standard model lifecycle defined in run_experiment. Prediction: This section defines a new type of model by creating a subclass of model. The prediction model learns to predict a particular outcome. It automatically optimizes parameters, selects features, selects an algorithm, and scores the results. Regressor model: The regressor model makes a numeric prediction. Use this model when the design specification of the datastory requires the AI microservice to give a numerical outut prediction. Classification model: The classification model makes a classification prediction. Use this model when the design specification of the datastory requires the AI microservice to give a categorical (text-based) outut prediction.","title":"Experiment"},{"location":"experiment/#run-ai-experiments","text":"An experiment trains and tests a machine-learning model. The code in this section runs a model through a complete lifecycle and saves the final model to the local drive. Run the code that defines a machine-learning model and its lifecycle. Design an experiment and execute it. Most of the work of choosing features and specific model parameters will be done automatically. The code will also automatically score each option and return the options with the best predictive performance. Below is a example for run_experiment() function. experiment_design = { #model options include ['regression()', 'classification()'] \"model\": ai.regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design) run_experiment(experiment_design): This function executes an experiment on selected model. Update experiment_design with parameters that fit your project. The data parameter should be same as the refined training data. The model parameter must be a model subclass. The labels parameter indicates the column of the data dataframe to be predicted. For the prediction model, the meta-data must describe the column to be predicted and the types for non-numeric columns. Further Information on Model Machine learning model provides the intelligent behavior that you will publish as a microservice. The code in this section provides you with options for the model. You must select a model capable of using df to learn the behavior specified in the design section of the datastory. Run this function by defining each model type, then choose the model most appropriate for your datastory. Each model adheres to the specifications of a model. This allows any of the models to run according to the standard model lifecycle defined in run_experiment. Prediction: This section defines a new type of model by creating a subclass of model. The prediction model learns to predict a particular outcome. It automatically optimizes parameters, selects features, selects an algorithm, and scores the results. Regressor model: The regressor model makes a numeric prediction. Use this model when the design specification of the datastory requires the AI microservice to give a numerical outut prediction. Classification model: The classification model makes a classification prediction. Use this model when the design specification of the datastory requires the AI microservice to give a categorical (text-based) outut prediction.","title":"Run AI Experiments"},{"location":"publish_microservice/","text":"Publish Microservice The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. In order to provide the information required to design the microservice, you must: create an Algorithmia account create an API key with BOTH \"Read & Write Data\" and \"Manage Algorithms\" permissions enabled create an algorithm user name trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } # publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) publish_microservice(microservice_design, trained_model): The code in this section prepares an execution environment for the microservice, builds a microservice using the machine-learning model, deploys the microservice into the execution environment, and publishes an API enpoint for the microservice. The work of creating the microservice and deploying it will be done automatically. The code will also automatically handle the source code reposity management. This code defines the parameters needed to build and delpoy a microservice based on the trained model. Update microservice_design with parameters appropriate for your project. The parameters must contain valid keys, namespaces, and model paths from Algorithmia.","title":"Publish Microservice"},{"location":"publish_microservice/#publish-microservice","text":"The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. In order to provide the information required to design the microservice, you must: create an Algorithmia account create an API key with BOTH \"Read & Write Data\" and \"Manage Algorithms\" permissions enabled create an algorithm user name trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } # publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) publish_microservice(microservice_design, trained_model): The code in this section prepares an execution environment for the microservice, builds a microservice using the machine-learning model, deploys the microservice into the execution environment, and publishes an API enpoint for the microservice. The work of creating the microservice and deploying it will be done automatically. The code will also automatically handle the source code reposity management. This code defines the parameters needed to build and delpoy a microservice based on the trained model. Update microservice_design with parameters appropriate for your project. The parameters must contain valid keys, namespaces, and model paths from Algorithmia.","title":"Publish Microservice"}]}