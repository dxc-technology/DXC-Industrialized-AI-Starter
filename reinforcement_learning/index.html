<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Reinforcement Learning - DXC Industrialized AI Starter</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Reinforcement Learning";
    var mkdocs_page_input_path = "reinforcement_learning.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> DXC Industrialized AI Starter</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Welcome</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../access_data/">Access Data</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../clean_data/">Clean Data</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../explore/">Visualization</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../data_pipeline/">Data Pipeline</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../experiment/">Experiment</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../model_explainability/">Model Explainability</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../publish_microservice/">Publish Microservice</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../logging/">Logging</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../data_sets/">Load Data</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Reinforcement Learning</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#set-up-the-development-environment">Set up the development environment</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#reinforcement-learning-basics">Reinforcement Learning Basics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#train-an-openai-gym-environment">Train an OpenAI Gym environment</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#models">Models</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#calling-the-reinforcement-learning-helper">Calling the Reinforcement Learning Helper</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#continuous-example">Continuous Example</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#discrete-example">Discrete Example</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../contributing_guide/">How to Contribute</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">DXC Industrialized AI Starter</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Reinforcement Learning</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="reinforcement-learning">Reinforcement Learning</h1>
<h2 id="set-up-the-development-environment">Set up the development environment</h2>
<p>This code installs all the packages you will need. Run it first. It should take 30 seconds or so to complete. If you get missing module errors later, it may be because you have not run this code. Restart the runtime/session after executing the below code.</p>
<pre><code class="python">! pip install git+https://github.com/dxc-technology/DXC-Industrialized-AI-Starter.git -qq
</code></pre>

<pre><code class="python">from dxc import rl
from gym import spaces
import numpy as np
import gym
</code></pre>

<h2 id="reinforcement-learning-basics">Reinforcement Learning Basics</h2>
<p>Reinforcement learning is machine learning using rewards given to an agent acting in an environment. Instead of learning from historical data, the agent learns how to maneuver through the environment by receiving positive or negative rewards depending on the actions that it takes.</p>
<h3 id="train-an-openai-gym-environment">Train an OpenAI Gym environment</h3>
<p>Gym is a toolkit used for reinforcement learning created by OpenAI that includes several premade environments to test your models on. View OpenAI Gym environments</p>
<h3 id="models">Models</h3>
<p>There are several models that can be used to train an agent in an environment. The model that needs to be used can be generally determined by the type of actions an agent can take.</p>
<p>If a discrete set of actions are given, DQN or SARSA can be used. A discrete set of actions are actions that you can count. Examples of this include 3 actions for Rock, Paper, Scissors or 2 actions in TicTacToe for X and O. If the action space were between 1 to 5, the actions would be 1, 2, 3, 4, and 5.</p>
<p>If a continous set of actions are given, DDPG can be used. A continuous set of actions are actions that you can measure. Examples of this include how fast a car should run (mph) or how likely a card will appear in a board game (%). If the action space were between 1 to 5, the actions would be all numbers between 1 to 5, including all decimal values.</p>
<h3 id="calling-the-reinforcement-learning-helper">Calling the Reinforcement Learning Helper</h3>
<p>rl_helper() accepts 2 parameters to run. The first parameter is the environment. The second parameter is the type of model the environment will train in.</p>
<p>There are extra parameters that can be defined, but aren't necessary. These are used to further refine your model. Listing them down and defining the default values in parenthesis after the parameter name, the parameters that can be set for both discrete and continuous environments are:</p>
<pre><code>- steps (50000): number of episodes that the model will run for
- saved_model_name (model): name of the saved model files
- visualize (False): boolean (True or False) if there is visualization for the model, set True to display it
</code></pre>
<p>The other parameters are only used for discrete environments.</p>
<pre><code>- test_steps (5): number of episodes the model will test
- critic_hidden_layers (3): number of critic hidden layers
- hidden_layers (3): number of hidden layers
</code></pre>
<p>To call a gym environment, simply use the gym.make() function and pass the name of the gym environment. You can get the name of the gym environment here.</p>
<h2 id="continuous-example">Continuous Example</h2>
<p>In the following example code, the Pendulum environment is used. This is an environment where the goal of the agent is to balance a pendulum upright. Unlike the cartpole example, the actions set for this environment are continuous, so DDPG will be used to train it.</p>
<pre><code class="python">env = gym.make(&quot;Pendulum-v0&quot;)
rl.rl_helper(env=env, 
             model_name=&quot;DDPG&quot;, 
             saved_model_name=&quot;pendulum_model&quot;, 
             steps=1000)
</code></pre>

<h2 id="discrete-example">Discrete Example</h2>
<p>In the following example code, the Cart Pole environment is used. This is an environment where the goal of the agent (which is a cart) is to balance a pole. The model used is DQN (discrete actions) since the actions that the cart can take is move left or right. SARSA may also be used since it is a discrete environment.</p>
<pre><code class="python">import gym

gym_env = gym.make(&quot;CartPole-v0&quot;)
rl.rl_helper(env=gym_env, 
             model_name=&quot;DQN&quot;, #SARSA
             steps=25000, 
             test_steps=5,
             visualize=False,
             saved_model_name=&quot;cartpole_model&quot;,
             critic_hidden_layers=2,
             hidden_layers=2)
</code></pre>

<h1 id="making-your-own-environment">Making your own environment</h1>
<p>Now that you've learned how to use the <code>rl_helper()</code>, you can explore how to create your own environment to train your own agent.</p>
<p>Each environment has 3 main parts: </p>
<p><code>__init__()</code> is where the variables describing the environment is initialized. This includes variables that define what an agent can do and observe. This also includes the variables that define how the environment looks as the agent is going through it. It is important to define what the agent can observe since this is what the agent will base his choices in his action on. For example, in an environment where a car is running, if the road is slippery, the agent might want to choose a slower speed. </p>
<p>In <code>__init__()</code>, it is important to define the following variables in the environment:</p>
<ul>
<li><code>action_space</code> initializes the number of actions that are defined. </li>
<li><code>observation_space</code> initializes the number of variables that is observed by the agent</li>
<li><code>total_reward</code> initializes and later stores the rewards that the agent has</li>
<li><code>isDone</code> initializes and stores whether or not the agent is done going through the environment</li>
</ul>
<p>The rest of the variables defined in init are variables that just describe what the environment looks like to the agent.</p>
<p><code>reset()</code> is where the variables of the environment are reset when the environment is ran again. Generally, it will just look like <code>__init__()</code> since we are just setting the variables to its original values again. This is done because the agent will train in the environment multiple times. To make sure each run independent of each other, the variables need to be reset to their original values. </p>
<p>It is important to note that <code>reset()</code> should return what the agent can observe.</p>
<p><code>step()</code> is where the steps that the agent takes through the environment is defined. </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../contributing_guide/" class="btn btn-neutral float-right" title="How to Contribute">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../data_sets/" class="btn btn-neutral" title="Load Data"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../data_sets/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../contributing_guide/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
